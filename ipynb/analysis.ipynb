{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages\n",
    "The below work is inspired by this link\n",
    "https://plot.ly/python/big-data-analytics-with-pandas-and-sqlite/\n",
    "\n",
    "The below work is also from:\n",
    "https://www.kaggle.com/prashantrenu/explore-large-data-python-sqlite-seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "\n",
    "import plotly.plotly as py # interactive graphing\n",
    "from plotly.graph_objs import Bar, Scatter, Marker, Layout\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rachel/Documents/kaggle/kaggle_instacart/ipynb'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default directory is /Users/rachel/Documents/kaggle/kaggle_instacart/ipynb\n",
      "The directory has now been changed to /Users/rachel/Documents/kaggle/kaggle_instacart\n",
      "parent dir is always /Users/rachel/Documents/kaggle/kaggle_instacart\n"
     ]
    }
   ],
   "source": [
    "print(\"The default directory is {}\".format(os.getcwd()))\n",
    "path = \"../\" # one level above of the current directory\n",
    "os.chdir(path) # convert the directory to one level above\n",
    "print(\"The directory has now been changed to {}\".format(os.getcwd()))\n",
    "\n",
    "parent_dir = os.getcwd()\n",
    "print(\"parent dir is always {}\".format(parent_dir))\n",
    "input_dir = \"/raw_data\" #the file is opened in a subfolder, need to go one level above and choose a different subfolder\n",
    "output_dir = \"/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw_data dir contains the below files:\n",
      "\n",
      "\n",
      "aisles.csv\n",
      "departments.csv\n",
      "order_products__prior.csv\n",
      "order_products__train.csv\n",
      "orders.csv\n",
      "products.csv\n",
      "sample_submission.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(\"Raw_data dir contains the below files:\")\n",
    "print (\"\\n\")\n",
    "print(check_output([\"ls\", parent_dir + input_dir]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an connection to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rachel/Documents/kaggle/kaggle_instacart/db/\n"
     ]
    }
   ],
   "source": [
    "db_dir = parent_dir + \"/db/\"\n",
    "print(db_dir)\n",
    "conn = create_engine('sqlite:///kaggle_instakart.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import csv data into sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aisles.csv', 'departments.csv', 'order_products__prior.csv', 'order_products__train.csv', 'orders.csv', 'products.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "csvFiles = [file for file in os.listdir(parent_dir + input_dir + \"/\") if file.find(\".csv\") != -1]\n",
    "print(csvFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'departments'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file= 'departments.csv'\n",
    "re.findall('.csv',file)\n",
    "file.replace('.csv','')\n",
    "#re.compile(file).sub(\"*.csv\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# for file in csvFiles:\n",
    "#     csv = pd.read_csv(parent_dir + input_dir + \"/\" + file, engine = 'c')\n",
    "#     tableName = file.replace('.csv','')\n",
    "#     csv.to_sql(tableName, conn, if_exists = 'replace', index = False)\n",
    "#     print (' Processing of Table {} ended'.format(tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# investigate the tables and join them together (use SQL only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aisles', 'departments', 'order_products__prior', 'order_products__train', 'orders', 'products', 'sample_submission']\n",
      "{}\n",
      "\n",
      "\n",
      "aisles\n",
      "   aisle_id                       aisle\n",
      "0         1       prepared soups salads\n",
      "1         2           specialty cheeses\n",
      "2         3         energy granola bars\n",
      "3         4               instant foods\n",
      "4         5  marinades meat preparation\n",
      "5         6                       other\n",
      "6         7               packaged meat\n",
      "7         8             bakery desserts\n",
      "8         9                 pasta sauce\n",
      "9        10            kitchen supplies\n",
      "\n",
      "\n",
      "departments\n",
      "   department_id       department\n",
      "0              1           frozen\n",
      "1              2            other\n",
      "2              3           bakery\n",
      "3              4          produce\n",
      "4              5          alcohol\n",
      "5              6    international\n",
      "6              7        beverages\n",
      "7              8             pets\n",
      "8              9  dry goods pasta\n",
      "9             10             bulk\n",
      "\n",
      "\n",
      "order_products__prior\n",
      "   order_id  product_id  add_to_cart_order  reordered\n",
      "0         2       33120                  1          1\n",
      "1         2       28985                  2          1\n",
      "2         2        9327                  3          0\n",
      "3         2       45918                  4          1\n",
      "4         2       30035                  5          0\n",
      "5         2       17794                  6          1\n",
      "6         2       40141                  7          1\n",
      "7         2        1819                  8          1\n",
      "8         2       43668                  9          0\n",
      "9         3       33754                  1          1\n",
      "\n",
      "\n",
      "order_products__train\n",
      "   order_id  product_id  add_to_cart_order  reordered\n",
      "0         1       49302                  1          1\n",
      "1         1       11109                  2          1\n",
      "2         1       10246                  3          0\n",
      "3         1       49683                  4          0\n",
      "4         1       43633                  5          1\n",
      "5         1       13176                  6          0\n",
      "6         1       47209                  7          0\n",
      "7         1       22035                  8          1\n",
      "8        36       39612                  1          0\n",
      "9        36       19660                  2          1\n",
      "\n",
      "\n",
      "orders\n",
      "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   2539329        1    prior             1          2                  8   \n",
      "1   2398795        1    prior             2          3                  7   \n",
      "2    473747        1    prior             3          3                 12   \n",
      "3   2254736        1    prior             4          4                  7   \n",
      "4    431534        1    prior             5          4                 15   \n",
      "5   3367565        1    prior             6          2                  7   \n",
      "6    550135        1    prior             7          1                  9   \n",
      "7   3108588        1    prior             8          1                 14   \n",
      "8   2295261        1    prior             9          1                 16   \n",
      "9   2550362        1    prior            10          4                  8   \n",
      "\n",
      "   days_since_prior_order  \n",
      "0                     NaN  \n",
      "1                    15.0  \n",
      "2                    21.0  \n",
      "3                    29.0  \n",
      "4                    28.0  \n",
      "5                    19.0  \n",
      "6                    20.0  \n",
      "7                    14.0  \n",
      "8                     0.0  \n",
      "9                    30.0  \n",
      "\n",
      "\n",
      "products\n",
      "   product_id                                       product_name  aisle_id  \\\n",
      "0           1                         Chocolate Sandwich Cookies        61   \n",
      "1           2                                   All-Seasons Salt       104   \n",
      "2           3               Robust Golden Unsweetened Oolong Tea        94   \n",
      "3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n",
      "4           5                          Green Chile Anytime Sauce         5   \n",
      "5           6                                       Dry Nose Oil        11   \n",
      "6           7                     Pure Coconut Water With Orange        98   \n",
      "7           8                  Cut Russet Potatoes Steam N' Mash       116   \n",
      "8           9                  Light Strawberry Blueberry Yogurt       120   \n",
      "9          10     Sparkling Orange Juice & Prickly Pear Beverage       115   \n",
      "\n",
      "   department_id  \n",
      "0             19  \n",
      "1             13  \n",
      "2              7  \n",
      "3              1  \n",
      "4             13  \n",
      "5             11  \n",
      "6              7  \n",
      "7              1  \n",
      "8             16  \n",
      "9              7  \n",
      "\n",
      "\n",
      "sample_submission\n",
      "   order_id     products\n",
      "0        17  39276 29259\n",
      "1        34  39276 29259\n",
      "2       137  39276 29259\n",
      "3       182  39276 29259\n",
      "4       257  39276 29259\n",
      "5       313  39276 29259\n",
      "6       353  39276 29259\n",
      "7       386  39276 29259\n",
      "8       414  39276 29259\n",
      "9       418  39276 29259\n"
     ]
    }
   ],
   "source": [
    "\"\"\"select from all the tables in db and inspect the top 5 rows\"\"\"\n",
    "dbTableNameList = [file.replace('.csv','') for file in csvFiles]\n",
    "print(dbTableNameList)\n",
    "\n",
    "tableNameDict = dict()\n",
    "print(tableNameDict)\n",
    "\n",
    "'''read the first 5 rows of each table'''\n",
    "for table in dbTableNameList:\n",
    "    sql_query = '''select * from ''' + table + ''';'''\n",
    "    print('\\n')\n",
    "    print(table)\n",
    "    t = pd.read_sql_query(sql_query, conn)\n",
    "    tableNameDict[table] = t\n",
    "    print(tableNameDict[table].head(10))\n",
    "\n",
    "# aisles = pd.read_sql_query(\"SELECT * FROM aisles LIMIT 5;\", conn)\n",
    "# print('Total aisles: {}'.format(aisles.shape[0]))\n",
    "# print(\"Aisles Table\\n\",aisles.head(),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ''' check the table individually and summarise in pandas'''\n",
    "\n",
    "# # data = tableNameDict['orders']\n",
    "# # #print(data.head())\n",
    "\n",
    "# # data.loc[data['eval_set'] == 'test']\n",
    "# # print(data.loc[data['user_id'] == 1])\n",
    "# #byeval = data.groupby('eval_set')\n",
    "# #print(byeval.sum())\n",
    "# # pd.DataFrame(orders.groupby(['product_name']).agg({'order_id':pd.Series.nunique})\n",
    "# #                          .rename(columns={'order_id':'cnt_ord_by_prod'})).reset_index()\n",
    "\n",
    "# # data2 = tableNameDict['order_products__prior']\n",
    "# # print(data2.loc[data2['order_id'] == 2774568])\n",
    "\n",
    "# # #print(data.loc[data['order_id'] == 2539329])\n",
    "\n",
    "# # data3 = tableNameDict['order_products__train']\n",
    "# # print(data3.loc[data3['order_id'] == 2398795])\n",
    "\n",
    "\n",
    "# # merge order table with all orders from users and when the order happened, since last order how long it took the user to order again\n",
    "# # also order_prior shows the products ordered by the order id\n",
    "# # user_id = 1\n",
    "\n",
    "# left = data.loc[data['user_id'] == 1]\n",
    "# right = data2 #prior\n",
    "# right2 = data3 # train\n",
    "# key = 'order_id'\n",
    "# result = pd.merge(left, right, on=key)\n",
    "# result2 = pd.merge(left, right2, on=key)\n",
    "# print(result)\n",
    "# print(result2)\n",
    "\n",
    "# # train is the latest order history with products, prior is before train, \n",
    "# # order is a summary for all users and their historical transactions (not products)\n",
    "# # pd.merge()\n",
    "# # order table is the current order (latest), order prior is the previous history of the customer order\n",
    "\n",
    "\n",
    "\n",
    "# #how many users in prior\n",
    "# # print(len(data['user_id'].unique()))\n",
    "# # print(data.head())\n",
    "\n",
    "\n",
    "# # prior should have all customers prior order \n",
    "\n",
    "# print(data2.head())\n",
    "# print(data3.head())\n",
    "# # # print(data.head())\n",
    "\n",
    "# # print(data.loc[data['eval_set'] == 'test'])\n",
    "\n",
    "\n",
    "# # no test user prior transactions?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # orders and prior inner join to get all train users prior history, then the train table\n",
    "# # train user inner join to orders and prior\n",
    "# #join order and prior first to have all users\n",
    "# #trian table to select train customers\n",
    "# # then the rest will be test cusotmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join tables\n",
    "- departments, aisles names can be joined to product table\n",
    "- prior, train and possibly test are to separate the product table\n",
    "- test table is used to predict next order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Combine aisles, departments and products to create goods table'''\n",
    "# start = dt.now()\n",
    "drop_goods_table=\"\"\" DROP TABLE IF EXISTS goods;\"\"\"\n",
    "conn.execute(drop_goods_table)\n",
    "\n",
    "join_prod_aisle_department = \"\"\"\n",
    "                            create table goods as                           \n",
    "                            select\n",
    "                                p.*,\n",
    "                                a.aisle_id as aisle_id_a,\n",
    "                                a.aisle,\n",
    "                                d.department_id as department_id_d,\n",
    "                                d.department\n",
    "\n",
    "                            from products p\n",
    "                                left join aisles a on p.aisle_id = a.aisle_id\n",
    "                                left join departments d on d.department_id = p.department_id;\"\"\"\n",
    "\n",
    "conn.execute(join_prod_aisle_department)\n",
    "\n",
    "goods = pd.read_sql_query(\"SELECT * FROM goods Limit 5;\", conn)\n",
    "# goods_step_time = dt.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''create a gigantic table for orders and prior products ordered by all customers'''\n",
    "drop_orders_prods_table=\"\"\" DROP TABLE IF EXISTS prod_combined;\"\"\"\n",
    "conn.execute(drop_orders_prods_table)\n",
    "\n",
    "join_prod_order_combined = \"\"\"\\\n",
    "                            create table prod_combined as                           \n",
    "                            select\n",
    "                                 p.*,\n",
    "                                 prior.product_id,\n",
    "                                 prior.add_to_cart_order,\n",
    "                                 prior.reordered\n",
    "                            from orders p\n",
    "                                left join order_products__prior prior\n",
    "                             on prior.order_id = p.order_id\n",
    "                                    ;\"\"\"\n",
    "\n",
    "conn.execute(join_prod_order_combined)\n",
    "\n",
    "prod_combined = pd.read_sql_query(\"SELECT * FROM prod_combined Limit 5;\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE INDEX index_order_id_prod_combined on prod_combined (order_id);\n",
      "CREATE INDEX index_user_id_prod_combined on prod_combined (user_id);\n",
      "CREATE INDEX index_product_id_prod_combined on prod_combined (product_id);\n"
     ]
    }
   ],
   "source": [
    "'''it is important to create index'''\n",
    "index_columns = [col for col in prod_combined.columns if col.find(\"_id\")>-1]\n",
    "for col in index_columns:\n",
    "    create_indexes=\"CREATE INDEX index_%s on prod_combined (%s);\" %(col+\"_prod_combined\",col)\n",
    "    print(create_indexes)\n",
    "    conn.execute(create_indexes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of    order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   2539329        1    prior             1          2                  8   \n",
      "1   2539329        1    prior             1          2                  8   \n",
      "2   2539329        1    prior             1          2                  8   \n",
      "3   2539329        1    prior             1          2                  8   \n",
      "4   2539329        1    prior             1          2                  8   \n",
      "\n",
      "  days_since_prior_order  product_id  add_to_cart_order  reordered  \n",
      "0                   None         196                  1          0  \n",
      "1                   None       12427                  3          0  \n",
      "2                   None       14084                  2          0  \n",
      "3                   None       26088                  4          0  \n",
      "4                   None       26405                  5          0  >\n"
     ]
    }
   ],
   "source": [
    "print(prod_combined.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''use pandas to create a gigantic table for orders and prior products ordered by all customers'''\n",
    "# df_orders = tableNameDict['orders']\n",
    "# df_prior = tableNameDict['order_products__prior']\n",
    "# df_train = tableNameDict['order_products__train']\n",
    "\n",
    "# key = 'order_id'\n",
    "# '''To get all historical order for both train and test users'''\n",
    "# df_all_hist = pd.merge(df_orders, df_prior, on = key , how = 'left')\n",
    "# print(df_all_hist.head(5))\n",
    "# print(len(df_all_hist.user_id.unique()))\n",
    "\n",
    "# '''To get train current orders'''\n",
    "# df_train_curr = pd.merge(df_orders, df_train, on = key,how = 'inner' )\n",
    "# train_users = df_train_curr['user_id'].unique()\n",
    "# print(train_users)\n",
    "# print(len(train_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''to get train user history only'''\n",
    "# df_all_train_hist = df_all_hist.loc[df_all_hist['user_id'].isin(train_users)]\n",
    "# print(df_all_train_hist.head())\n",
    "# print(len(df_all_train_hist.user_id.unique()))\n",
    "\n",
    "# '''to get test user history only'''\n",
    "# df_all_test_hist = df_all_hist.loc[df_all_hist['user_id'].isin(train_users) == False]\n",
    "# print(df_all_test_hist.head())\n",
    "# print(len(df_all_test_hist.user_id.unique()))\n",
    "\n",
    "# # do not use this one, too slow!\n",
    "# # df_all_hist['train_test_flag'] = df_all_hist['user_id'].map(lambda x: 'train' if x in train_users else 'test')\n",
    "# # print(df_all_hist.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''create a table with everything'''\n",
    "drop_prior_datamart_table=\"\"\" DROP TABLE IF EXISTS prior_datamart;\"\"\"\n",
    "conn.execute(drop_prior_datamart_table)\n",
    "join_prior_datamart_sql = \"\"\"\\\n",
    "    CREATE TABLE prior_datamart AS\n",
    "    SELECT o.*, gd.product_id, gd.product_name,gd.department,gd.aisle\n",
    "    FROM prod_combined o\n",
    "    INNER JOIN goods gd\n",
    "    ON o.product_id = gd.product_id;\n",
    "    \"\"\"\n",
    "conn.execute(join_prior_datamart_sql)\n",
    "prior_datamart = pd.read_sql_query(\"SELECT * FROM prior_datamart Limit 5;\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_id', 'user_id', 'product_id']\n",
      "CREATE INDEX index_order_id_prior_datamart on prior_datamart (order_id);\n",
      "CREATE INDEX index_user_id_prior_datamart on prior_datamart (user_id);\n",
      "CREATE INDEX index_product_id_prior_datamart on prior_datamart (product_id);\n",
      "prior_datamart Table \n",
      "    order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   3139998      138    prior            28          6                 11   \n",
      "1   1977647      138    prior            30          6                 17   \n",
      "2    389851      709    prior             2          0                 21   \n",
      "3    652770      764    prior             1          3                 13   \n",
      "4   1813452      764    prior             3          4                 17   \n",
      "\n",
      "   days_since_prior_order  product_id  add_to_cart_order  reordered  \\\n",
      "0                     3.0           1                  5          0   \n",
      "1                    20.0           1                  1          1   \n",
      "2                     6.0           1                 20          0   \n",
      "3                     NaN           1                 10          0   \n",
      "4                     9.0           1                 11          1   \n",
      "\n",
      "                 product_name department          aisle  \n",
      "0  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "1  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "2  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "3  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "4  Chocolate Sandwich Cookies     snacks  cookies cakes   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prior_datamart.drop('product_id:1', axis=1, inplace=True)\n",
    "\n",
    "index_columns = [col for col in prod_combined.columns if col.find(\"_id\")>-1]\n",
    "\n",
    "print(index_columns)\n",
    "\n",
    "for col in index_columns:\n",
    "    create_indexes=\"CREATE INDEX index_%s on prior_datamart (%s);\" %(col+\"_prior_datamart\",col)\n",
    "    print(create_indexes)\n",
    "    conn.execute(create_indexes)        \n",
    "\n",
    "#print('\\nTotal Time taken to delete and create prior_datamart table {}\\n'.format((prior_datamart_time-orders_combined_time).total_seconds()))\n",
    "print(\"prior_datamart Table \\n\", prior_datamart.head(10),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del goods, orders_combined,prior_datamart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   3139998      138    prior            28          6                 11   \n",
      "1   1977647      138    prior            30          6                 17   \n",
      "2    389851      709    prior             2          0                 21   \n",
      "3    652770      764    prior             1          3                 13   \n",
      "4   1813452      764    prior             3          4                 17   \n",
      "\n",
      "   days_since_prior_order  product_id  add_to_cart_order  reordered  \\\n",
      "0                     3.0           1                  5          0   \n",
      "1                    20.0           1                  1          1   \n",
      "2                     6.0           1                 20          0   \n",
      "3                     NaN           1                 10          0   \n",
      "4                     9.0           1                 11          1   \n",
      "\n",
      "                 product_name department          aisle  \n",
      "0  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "1  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "2  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "3  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "4  Chocolate Sandwich Cookies     snacks  cookies cakes  \n",
      "(5, 13)\n"
     ]
    }
   ],
   "source": [
    "print(prior_datamart.head())\n",
    "print(prior_datamart.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the products\n",
    "- the analysis will cover univariate and bivariate analysis\n",
    "- plots are included\n",
    "- the analysis will cover both product top line and order behaviour analysis\n",
    "\n",
    "What else?\n",
    "- how many products the super market are selling under how many departments\n",
    "- where products are placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py # interactive graphing\n",
    "from plotly.graph_objs import Bar, Scatter, Marker, Layout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUnivariate Analysis\\n1. When do people order (Distribution of Time of Day) ?\\n2. Day of Week (Distribution of day_of_week)?\\n3. When do they order again (Distribution of Time Since Prior Order)?\\n4. How many prior orders are there (Distribution of Reorders)?\\n\\n5. how often people order for similar products, say products from the same department\\n6. in train data, we have 13000 customers' order history, for every product they bought, then we can use that to predict what other products they reorder in the curr history the latest\\n7. while for the test cutomers, they also have history, maybe people who are similar shoul dbe clustered in the same group, then build a model for each cluster\\n8. how similar train and test customers are???\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Univariate Analysis\n",
    "1. When do people order (Distribution of Time of Day) ?\n",
    "2. Day of Week (Distribution of day_of_week)?\n",
    "3. When do they order again (Distribution of Time Since Prior Order)?\n",
    "4. How many prior orders are there (Distribution of Reorders)?\n",
    "\n",
    "5. how often people order for similar products, say products from the same department\n",
    "6. in train data, we have 13000 customers' order history, for every product they bought, then we can use that to predict what other products they reorder in the curr history the latest\n",
    "7. while for the test cutomers, they also have history, maybe people who are similar shoul dbe clustered in the same group, then build a model for each cluster\n",
    "8. how similar train and test customers are???\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "department          \n",
       "alcohol        count    1.409820e+05\n",
       "               mean     1.034089e+01\n",
       "               std      9.024931e+00\n",
       "               min      0.000000e+00\n",
       "               25%      4.000000e+00\n",
       "               50%      7.000000e+00\n",
       "               75%      1.400000e+01\n",
       "               max      3.000000e+01\n",
       "babies         count    4.013150e+05\n",
       "               mean     1.000447e+01\n",
       "               std      8.115978e+00\n",
       "               min      0.000000e+00\n",
       "               25%      4.000000e+00\n",
       "               50%      7.000000e+00\n",
       "               75%      1.300000e+01\n",
       "               max      3.000000e+01\n",
       "bakery         count    1.101246e+06\n",
       "               mean     1.109581e+01\n",
       "               std      8.750257e+00\n",
       "               min      0.000000e+00\n",
       "               25%      5.000000e+00\n",
       "               50%      8.000000e+00\n",
       "               75%      1.500000e+01\n",
       "               max      3.000000e+01\n",
       "beverages      count    2.522511e+06\n",
       "               mean     1.105337e+01\n",
       "               std      8.807469e+00\n",
       "               min      0.000000e+00\n",
       "               25%      5.000000e+00\n",
       "               50%      8.000000e+00\n",
       "                            ...     \n",
       "personal care  std      9.240223e+00\n",
       "               min      0.000000e+00\n",
       "               25%      5.000000e+00\n",
       "               50%      8.000000e+00\n",
       "               75%      1.600000e+01\n",
       "               max      3.000000e+01\n",
       "pets           count    9.110100e+04\n",
       "               mean     1.186807e+01\n",
       "               std      8.912416e+00\n",
       "               min      0.000000e+00\n",
       "               25%      5.000000e+00\n",
       "               50%      9.000000e+00\n",
       "               75%      1.600000e+01\n",
       "               max      3.000000e+01\n",
       "produce        count    8.894907e+06\n",
       "               mean     1.085621e+01\n",
       "               std      8.647263e+00\n",
       "               min      0.000000e+00\n",
       "               25%      5.000000e+00\n",
       "               50%      7.000000e+00\n",
       "               75%      1.400000e+01\n",
       "               max      3.000000e+01\n",
       "snacks         count    2.706641e+06\n",
       "               mean     1.094788e+01\n",
       "               std      8.664682e+00\n",
       "               min      0.000000e+00\n",
       "               25%      5.000000e+00\n",
       "               50%      8.000000e+00\n",
       "               75%      1.400000e+01\n",
       "               max      3.000000e+01\n",
       "Name: days_since_prior_order, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# department order interval by customers: how often do they buy, mean and std; interval of purchase mean and std\n",
    "prior_datamart = pd.read_sql_query(\"SELECT * FROM prior_datamart;\", conn)\n",
    "prior_datamart_2 = prior_datamart.loc[prior_datamart['days_since_prior_order'] != None]\n",
    "byDepartment = prior_datamart_2.groupby('department')\n",
    "byDepartment['days_since_prior_order'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
